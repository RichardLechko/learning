# 6.3 Floating-Point Constants and Precision

Floating-point constants have fundamentally different storage and performance characteristics compared to integer constants. Understanding IEEE 754 representation and x86-64 floating-point architecture is crucial for writing efficient numerical code.

## 6.3.1 Floating-Point Constant Storage

Unlike integer constants that can be embedded as immediate values, floating-point constants require special handling.

**Integer vs Floating-Point Constants:**

```c
int i = 1000;        // Can be immediate value: movl $1000, %eax
double d = 1.0;      // Must be stored in memory section
```

**Why Floating-Point Constants Require Memory Storage:**
- x86-64 instructions have limited immediate operand sizes
- IEEE 754 double precision requires 64 bits
- Complex bit patterns don't fit in instruction encoding

## 6.3.2 Assembly Representation of Floating-Point Constants

**Source Code:**
```c
static double OnePointZero_c = 1.0;

int main(int argc, char **argv) {
    static double i;
    i = OnePointZero_c;
    return 0;
}
```

**Assembly Output:**
```assembly
# Floating-point constant storage in .rodata section
.section .rodata
.align 8
.LC0:
    .long 0              # Lower 32 bits of 1.0
    .long 1072693248     # Upper 32 bits of 1.0

# Variable initialization
main:
    movsd .LC0(%rip), %xmm0     # Load 1.0 from constant pool
    movsd %xmm0, i.0(%rip)      # Store to variable i
```

## 6.3.3 IEEE 754 Representation

**Double Precision (64-bit) Format:**
```
[Sign:1][Exponent:11][Mantissa:52]
```

**Single Precision (32-bit) Format:**
```
[Sign:1][Exponent:8][Mantissa:23]
```

### Decoding Assembly Constants

**Converting Assembly to Decimal:**

From your code example:
```assembly
.LC0:
    .long 0              # Lower 32 bits
    .long 1072693248     # Upper 32 bits
```

**Step-by-step conversion:**
1. **Combine 32-bit parts**: `1072693248` (high) + `0` (low)
2. **Convert to hex**: `1072693248` = `0x3FF00000`
3. **Full 64-bit value**: `0x3FF0000000000000`
4. **IEEE 754 breakdown**:
   - Sign: `0` (positive)
   - Exponent: `0x3FF` = 1023 (biased), actual = 1023-1023 = 0
   - Mantissa: `0x0000000000000` (1.0 in normalized form)
5. **Result**: 1.0

## 6.3.4 Floating-Point Instructions and Registers

**x86-64 Floating-Point Architecture:**

```assembly
# SSE/AVX registers for floating-point operations
movss %xmm0, %xmm1      # 32-bit float move
movsd %xmm0, %xmm1      # 64-bit double move
addss %xmm1, %xmm0      # 32-bit float addition  
addsd %xmm1, %xmm0      # 64-bit double addition
```

**Register Usage:**
- `%xmm0` - `%xmm15`: 128-bit registers for floating-point
- Can hold: 1 double, 2 floats, or vector data
- Separate from integer registers (`%rax`, `%rbx`, etc.)

## 6.3.5 Performance Implications

### Memory Access Patterns

**Floating-Point Constant Access:**
```c
double result = 3.14159 * radius * radius;
```

**Assembly Pattern:**
```assembly
movsd .LC_PI(%rip), %xmm0    # Load PI from memory (cache miss possible)
movsd radius(%rip), %xmm1    # Load radius 
mulsd %xmm1, %xmm0           # PI * radius
mulsd %xmm1, %xmm0           # * radius again
movsd %xmm0, result(%rip)    # Store result
```

**Performance Considerations:**
- **Cache locality**: Constants in `.rodata` section
- **Memory bandwidth**: Each constant access uses memory bus
- **Instruction cache**: Constant addresses increase instruction size

### Float vs Double Performance

**Modern x86-64 Performance:**
```c
// Similar performance on modern CPUs
float f_calc = 3.14f * 2.0f;     # Single precision
double d_calc = 3.14 * 2.0;      # Double precision

// But different memory usage
float array[1000];               # 4KB
double array[1000];              # 8KB (may cause more cache misses)
```

## 6.3.6 Precision and Accuracy Issues

### Representation Limits

**Exact vs Approximate Representation:**
```c
double exact = 0.5;        // Exact: 2^-1
double exact2 = 0.25;      // Exact: 2^-2
double approx = 0.1;       // Approximate! Cannot be exactly represented
```

**Precision Demonstration:**
```c
float f = 16777216.0f;     // 2^24 - last exactly representable float
printf("f = %.1f\n", f);   // 16777216.0

f = f + 1.0f;              
printf("f + 1 = %.1f\n", f); // Still 16777216.0! Precision lost
```

### Assembly Evidence of Precision Loss

**Source Code:**
```c
float test_precision() {
    float large = 16777216.0f;
    float result = large + 1.0f;
    return result;
}
```

**Assembly Output:**
```assembly
# The addition still occurs, but precision is lost in IEEE 754 format
movss .LC_LARGE(%rip), %xmm0    # Load 16777216.0f
movss .LC_ONE(%rip), %xmm1      # Load 1.0f  
addss %xmm1, %xmm0              # Add (but result rounds back to original)
```

## 6.3.7 Floating-Point Optimization Techniques

### Constant Folding

**Compile-Time Evaluation:**
```c
double area = 3.14159 * 10.0 * 10.0;  // Computed at compile time
```

**Assembly Result:**
```assembly
movsd .LC_COMPUTED(%rip), %xmm0  # Direct load of 314.159
movsd %xmm0, area(%rip)
```

### Fast Math Optimizations

**Compiler Flags:**
```bash
gcc -ffast-math code.c        # Aggressive floating-point optimizations
gcc -fno-math-errno code.c    # Don't set errno for math functions
```

**Impact on Constants:**
```c
// With -ffast-math, compiler may optimize:
double result = x * 1.0;      // Becomes: double result = x;
double zero = x - x;          // Becomes: double zero = 0.0;
```

## 6.3.8 Special Floating-Point Values

### IEEE 754 Special Cases

**Infinity and NaN:**
```c
double pos_inf = 1.0 / 0.0;    // +∞
double neg_inf = -1.0 / 0.0;   // -∞  
double not_num = 0.0 / 0.0;    // NaN
double neg_zero = -0.0;        // -0 (distinct from +0)
```

**Assembly Representation:**
```assembly
.section .rodata
.align 8
.POS_INF:
    .long 0x00000000, 0x7FF00000    # +∞
.NEG_INF: 
    .long 0x00000000, 0xFFF00000    # -∞
.NAN_VAL:
    .long 0x00000001, 0x7FF00000    # NaN (one possible representation)
```

### Denormal Numbers and Performance

**Denormal Number Problem:**
```c
double tiny = 1e-320;          // May be denormal (very slow!)

// Performance cliff - 100x slower than normal numbers
for (int i = 0; i < 1000000; i++) {
    tiny = tiny * 2.0;         # Each operation extremely slow
}
```

**Solution - Flush to Zero:**
```c
#include <xmmintrin.h>

// Enable flush-to-zero mode for performance
_MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);
_MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);
```

## 6.3.9 Best Practices for Floating-Point Constants

### Constant Organization

**Centralized Constant Definitions:**
```c
// constants.h
extern const double PI;
extern const double E;
extern const double GOLDEN_RATIO;

// constants.c  
const double PI = 3.14159265358979323846;
const double E = 2.71828182845904523536;
const double GOLDEN_RATIO = 1.61803398874989484820;
```

### Type-Specific Literals

**Explicit Type Suffixes:**
```c
float f_pi = 3.14159f;         // Float literal (32-bit)
double d_pi = 3.14159;         // Double literal (64-bit)
long double ld_pi = 3.14159L;  // Long double literal (80/128-bit)
```

### Precision-Aware Comparisons

**Never Use Direct Equality:**
```c
// WRONG
if (calculated_value == 0.1) { /* May never be true */ }

// CORRECT  
#define EPSILON 1e-10
if (fabs(calculated_value - 0.1) < EPSILON) { /* Safe comparison */ }
```

## 6.3.10 Performance Summary

| Operation | Cycles (approx.) | Notes |
|-----------|------------------|-------|
| Integer constant load | 1 | Immediate value in instruction |
| Float constant load | 3-4 | Memory load from .rodata |
| Float arithmetic | 3-5 | Modern CPUs have fast FPUs |
| Double arithmetic | 3-5 | Similar to float on x86-64 |
| Denormal operations | 50-100+ | Avoid in performance-critical code |

Understanding these floating-point characteristics allows you to write numerical code that is both accurate and performant, avoiding common pitfalls while leveraging modern CPU floating-point capabilities.